{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.tabular.all import *\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom fastai.callback import *\nfrom tqdm.notebook import tqdm\nimport sys\nsys.path.append('/kaggle/input/iterative-stratification/iterative-stratification-master')\n#print(sys.path)\n#!ls ../input/iterative-stratification/iterative-stratification-master\n#from iterstrat import ml_stratifiers\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n#from ml_stratifiers import MultilabelStratifiedKFold\nimport copy\nfrom torch.distributions.beta import Beta\nfrom sklearn.preprocessing import QuantileTransformer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 50)\nKAGGLE = True\nTRAIN = True\nINFERENCE = True\nPATH = '../input/lish-moa/' if KAGGLE else None\n\nprint(PATH)\ntest_features = pd.read_csv(PATH + 'test_features.csv')\ntrain_features = pd.read_csv(PATH  + 'train_features.csv')\ntrain_targets_scored = pd.read_csv(PATH + 'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(PATH + 'train_targets_nonscored.csv')\nsample_submission = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\ndrug_ids = pd.read_csv(PATH + 'train_drug.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genes_cols = [col for col in train_features.columns if col.startswith('g-')]\ncells_cols = [col for col in train_features.columns if col.startswith('c-')]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_and_test_genes_features = pd.concat([train_features[genes_cols],test_features[genes_cols]])\ntrain_and_test_cell_features = pd.concat([train_features[cells_cols],test_features[cells_cols]])\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nn_comp_genes = 600\npca_genes = PCA(n_components=n_comp_genes)\npca_names_genes = ['pca_genes_' + str(i) for i in range(300)]#range(n_comp_genes)]\npca_genes.fit(train_and_test_genes_features)\nprint(pca_genes.explained_variance_ratio_[:300].sum())\nn_comp_cells = 60\npca_cells = PCA(n_components=n_comp_cells)\npca_names_cells = ['pca_cells_' + str(i) for i in range(n_comp_cells)]\npca_cells.fit(train_and_test_cell_features)\nprint(pca_cells.explained_variance_ratio_.sum())\n\n#np.sum(pca.explained_variance_ratio_),pca.explained_variance_ratio_\ntrain_pca_features_genes = pca_genes.transform(train_features[genes_cols])[:, :300]\ntest_pca_features_genes = pca_genes.transform(test_features[genes_cols])[:,:300]\ntrain_features[pca_names_genes] = train_pca_features_genes\ntest_features[pca_names_genes] = test_pca_features_genes\n\ntrain_pca_features_cells= pca_cells.transform(train_features[cells_cols])\ntest_pca_features_cells = pca_cells.transform(test_features[cells_cols])\ntrain_features[pca_names_cells] = train_pca_features_cells\ntest_features[pca_names_cells] = test_pca_features_cells\n\n#tmp_df = pd.DataFrame(columns = pca_names)\n#tmp_df[pca_names] = train_pca_features\n#train_features.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_scored_cols = train_targets_scored.columns.tolist()[1:]\ntarget_nonscored_cols = train_targets_nonscored.columns.tolist()[1:]\ntrain_df = train_features.merge(train_targets_scored,on='sig_id',how='left').merge(\n           train_targets_nonscored,on='sig_id',how='left').merge(\n           drug_ids,on='sig_id',how='left')\n \ndf = train_df.sample(frac=1.,random_state=2020)\n\ndf['kfold_scored'] = -1\nkf = MultilabelStratifiedKFold(n_splits=5)\ny = df[target_scored_cols + ['drug_id']].values\nfor fold, (t_,v_) in enumerate(kf.split(X=df,y=y)):\n    df.loc[v_,'kfold_scored'] = fold\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold    \ndf['kfold_nonscored'] = -1\nkf = MultilabelStratifiedKFold(n_splits=5)\n#kf = StratifiedKFold(n_splits=5)\ny = df[target_nonscored_cols + target_scored_cols + ['drug_id']].values\nfor fold, (t_,v_) in enumerate(kf.split(X=df,y=y)):\n    df.loc[v_,'kfold_nonscored'] = fold\n    \n#???df.to_csv    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sig_ids = test_features[test_features['cp_type'] == 'ctl_vehicle']['sig_id'].values\n#len(target_cols)\ncat_names =  ['cp_type', 'cp_time', 'cp_dose']# + ['n']\ncont_names = [c for c in train_features.columns if c not in cat_names \n               and c != 'sig_id'\n               and c != 'drug_id']  #+ pca_names#+ ['n']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = pd.concat([train_features[cont_names],test_features[cont_names]])\nmean  = all_features.mean().values\nstd = all_features.std().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn import BCELoss\n\n\nclass LabelSmoothingCrossEntropyEgm(nn.Module):\n    y1 = None\n    lam = None\n    def __init__(self, smoothing:float=0.005, reduction='mean'):\n        super().__init__()\n        self.smoothing,self.reduction = smoothing,reduction\n        self.f = BCELoss(reduction = 'none')\n        \n    \n    def forward(self, output, target):\n        \n        c = output.size()[-1]\n        target=target.float()\n        with torch.no_grad():\n            target = target * (1.0 - self.smoothing) + 0.08 * self.smoothing\n        bce_loss = self.f(output, target)\n              \n        if (self.y1 == None):\n            loss = bce_loss.mean()\n        \n            return loss\n        else:\n            target1=self.y1.float()\n            with torch.no_grad():\n                 target1 = target1 * (1.0 - self.smoothing) + 0.08 * self.smoothing\n            bce_loss1 = self.f(output, target1)\n            all_loss = torch.lerp(bce_loss, bce_loss1, self.lam)\n            return all_loss.mean()\n    \n    \n #0.001 0.05   \n #0.005 0.08\nclass ChangeLoss(Callback):\n    _order = 90 #Runs after normalization and cuda\n    \n    valid_loss = BCELossFlat()\n    train_loss = LabelSmoothingCrossEntropyEgm()\n    \n    def before_batch(self, **kwargs):\n        val_condition = (self.learn.dls[1] == self.learn.dl)\n        \n        if (val_condition):\n            self.learn.loss_func = self.valid_loss\n        else:\n            self.learn.loss_func = self.train_loss\nclass NormalizeCallback(Callback):\n    def before_batch(self, **kwargs):\n         \n        \n        a_cat, a_cont = self.learn.xb\n        x_cont = a_cont.cpu()\n        #x_cont[:, -360:] = (a_cont[:,-360:].cpu() - pca_mean)/pca_std\n        x_cont = (a_cont.cpu() - mean)/std\n        x_cont = x_cont.float()\n        x_cont = x_cont.to(a_cont.device)\n        \n        self.learn.xb = (a_cat, x_cont)\na, b = None,None\nclass CatMixUp(Callback):\n    #run_after,run_valid = [Normalize],False\n    run_before = [Normalize]\n    def __init__(self, alpha=0.4): self.distrib = Beta(tensor(alpha), tensor(alpha))\n    \n    def before_batch(self, **kwargs):\n        ret_condition = (self.learn.dls[0] != self.learn.dl)\n        \n        if (ret_condition): #if not train do nothing\n            return \n        #global a,b\n        a=self.learn.xb \n        b, =self.learn.yb\n        #print(3/0)\n        lam = self.distrib.sample((self.y.size(0),)).unsqueeze(-1)\n\n        a_cat, a_cont = a\n        x_cat = a_cat.detach().clone()\n        x_cont = a_cont.detach().clone()\n        y=b.detach().clone()\n\n        codes = x_cat[:,0]*16 + x_cat[:,1]*4 +x_cat[:,2]# categories common code\n        uniq_codes = torch.unique(codes)\n        for code in uniq_codes:\n            indexes = (codes==code).nonzero().view(-1) # at which indexes\n            ind_perm = torch.randperm(len(indexes))\n            x_cont[indexes] = x_cont[indexes[ind_perm]]\n            y[indexes] = y[indexes[ind_perm]]\n        \n        x_cont = x_cont.to(a_cont.device)\n        y=y.to(a_cont.device)\n        lam=lam.to(a_cont.device)\n        out_cont = torch.lerp(a_cont, x_cont, lam) \n        #####772+100+300+60\n        out_cont[:,772+100:772+100+300]=torch.tensor(pca_genes.transform(out_cont[:,:772].tolist())[:,:300])\n        out_cont[:,-60:]=torch.tensor(pca_cells.transform(out_cont[:, 772:772+100].tolist()))\n        #####\n        \n        \n        \n        out_y = torch.lerp(b.float(), y.float(), lam)\n        out_cont = out_cont.to(a_cont.device)\n        out_y = out_y.to(b.device)\n        self.learn.loss_func.lam = lam\n        self.learn.loss_func.y1 = y\n        self.learn.xb = (a_cat, out_cont)\n        self.learn.yb = (out_y,)        \n\nseeds = [42, 7, 9, 13, 37, 11, 5, 29, 31, 37, 41, 53]          \nBCE_LOSS = BCELoss(reduction='mean')#????","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(fold, target_names, \n             procs = [Categorify, FillMissing],\n             cat_names = cat_names,\n             cont_names = cont_names):\n    val_idx = df[df.kfold_nonscored==fold].index\n    dls = TabularDataLoaders.from_df(df, path=PATH, \n                                        y_names=target_names,\n                                        cat_names = cat_names,\n                                        cont_names = cont_names,\n                                        procs = procs,#, Normalize],\n                                        valid_idx=val_idx,                                        \n                                        bs=64)\n    return dls\ndef get_cbs(do_mixup):\n    ncb = NormalizeCallback()\n    ch_loss_cb = ChangeLoss()    \n    if do_mixup:\n        return  [ch_loss_cb,CatMixUp(),ncb]\n        #return  [ch_loss_cb,ncb] <- when I talked about scores I've changed exactly this line....\n    return [ncb, ch_loss_cb]\n\ntest_scores = []\nresults = []\n\ndef do_train_and_inf(num_iters=1, do_train=True, do_inference=False, cbs=get_cbs(False), \n                     lr=9e-3, epochs=5, target_names=target_scored_cols, \n                     use_pretr=False, pretr_model=False, file_name = 'something'):\n    global test_scores\n    global results\n    model = None\n    for ind in tqdm(range(num_iters)):\n        seed_everything(seeds[ind])\n        i = ind % 5 \n        dls = get_data(i, target_names = target_names) # Data\n        model_dir = '/kaggle/working/' if TRAIN else '/kaggle/input/fastai-egm'\n        config = tabular_config(ps=0.2)\n        learn = tabular_learner(dls , y_range=(0,1), \n                                layers = [1024, 512, 512, 256],                                \n                                loss_func = LabelSmoothingCrossEntropyEgm(),\n                                config=config,\n                                model_dir=model_dir,\n                            cbs=cbs\n                           ) # Model\n        model = learn.model\n    \n        if (use_pretr):\n            print(\"Will change model\")\n            remember = learn.model.layers[-2]    \n            learn.model = copy.deepcopy(pretr_model)#.load_state_dict(torch.load('/kaggle/working/pretrained'))\n            learn.model.layers[-2] = remember\n\n        name = file_name + str(ind)\n    \n        cb = SaveModelCallback(monitor='valid_loss',fname=name ,mode='min') # Callbacks    \n        if (do_train):\n           \n            learn.fit_one_cycle(epochs, lr=slice(lr/(2.6**4),lr), cbs=cb) # Training\n            results = results + [learn.recorder.loss.value.item()]\n            \n        if (do_inference):\n            learn.load(name) # Load best model\n                \n            test_dl = learn.dls.test_dl(test_features)#learn.dls.valid#learn.valid_dllearn.dls.test_dl(test_features)\n            sub = learn.get_preds(dl=test_dl) # prediction\n            test_scores.append(sub[0].numpy())\n    \n    #if TRAIN:\n        #learn.export('/kaggle/working/'+name+'.pkl') # export model\n    \n    return model\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = do_train_and_inf(num_iters=1, do_train = True, do_inference=False, \n                         cbs = get_cbs(False), lr = 9e-3, epochs = 5, \n                        target_names = target_nonscored_cols + target_scored_cols,\n                        use_pretr = False, pretr_model = None,\n                        file_name = 'pretrain_')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"do_train_and_inf(num_iters=10, do_train = True, do_inference=True, \n                cbs = get_cbs(True), lr = 9e-3, epochs = 10, \n                target_names = target_scored_cols,\n                use_pretr = True, pretr_model = model,\n                file_name = 'pretrain_')\ntest_sc = np.array(test_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for r in results:\n    print(str(r).replace('.',','))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_prds = test_sc.mean(axis=0)\nsubmission = sample_submission.copy()\nsubmission[target_scored_cols] = avg_prds\nsubmission.loc[submission['sig_id'].isin(test_features.loc[test_features['cp_type'] =='ctl_vehicle', 'sig_id']), train_targets_scored.columns[1:]] = 0\n#submission['atp-sensitive_potassium_channel_antagonist'] = 0\n#submission['erbb2_inhibitor'] = 0\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}