{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from scipy.signal import find_peaks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task1.\n",
    "\n",
    "First, we can generate test data(GENERATE_TEST_DATA = True) (or can just put *.npy files)\n",
    "\n",
    "Second, we solve task per se. By fft we get top frequencies for mixtures and pure signals and compare top frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44100  # Гц #todo check it's not important\n",
    "DURATION = 5  # Секунды\n",
    "\n",
    "GENERATE_TEST_DATA = True\n",
    "\n",
    "MIXTURE_LENGTH = 10000\n",
    "SIGNAL_LENGTH = 1024\n",
    "MIXTURES_NUM = 1000\n",
    "SIGNALS_NUM = 100\n",
    "\n",
    "def get_wave(freq1, freq2, sample_rate, duration, a = 1, b = 0, c = 0, d = 0):\n",
    "    x = np.linspace(0, duration, sample_rate*duration, endpoint=False)[:MIXTURE_LENGTH]\n",
    "    frequency1 = (x * freq1)\n",
    "    frequency2 = (x * freq2)\n",
    "    y = a* np.sin((2 * np.pi) * frequency1) + b* np.sin((2*np.pi + np.pi*c) * frequency2)    \n",
    "    return x, y\n",
    "\n",
    "def generate_data():\n",
    "    y = []\n",
    "    params = []\n",
    "    for i in range(SIGNALS_NUM):\n",
    "\n",
    "        params.append(\n",
    "            (100 + 50*i, \n",
    "             5000 +  50*i,\n",
    "             random.randint(2, 6),\n",
    "             random.randint(2, 6),\n",
    "             0,#random.randint(2, 2),\n",
    "             0))#random.random()))\n",
    "    shifts = [random.randint(0, 9000)] * 100\n",
    "    ratios = [0.3] * 1000\n",
    "    signals = []\n",
    "    for i in range(MIXTURES_NUM):\n",
    "        p = params[i % 100]\n",
    "        _,wave = get_wave(p[0], p[1], SAMPLE_RATE, DURATION, p[2], p[3], p[4], p[5])\n",
    "        len(wave)\n",
    "        if (i < SIGNALS_NUM):\n",
    "            signals.append(wave[shifts[i]:(shifts[i] + SIGNAL_LENGTH)])\n",
    "        _,noise = get_wave(random.randint(10000, 18000),\n",
    "                           random.randint(10000, 18000),\n",
    "                           SAMPLE_RATE, DURATION,\n",
    "                           1, \n",
    "                           1,\n",
    "                           0,\n",
    "                           0)\n",
    "        y.append( wave + ratios[i] * noise)\n",
    "\n",
    "    with open('signals.npy', 'wb') as f:\n",
    "        np.save(f,signals)\n",
    "    with open('mixtures.npy', 'wb') as f:\n",
    "        np.save(f,y)    \n",
    "\n",
    "if GENERATE_TEST_DATA:\n",
    "    generate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main method do_task1()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fxy(signal):\n",
    "    yf = rfft(signal)\n",
    "    xf = rfftfreq(len(signal), 1 / SAMPLE_RATE)\n",
    "    return xf,yf\n",
    "\n",
    "def get_indexes(xf, yf, length):   \n",
    "    return find_peaks(np.abs(yf)/length, height=0.01)[0]\n",
    "\n",
    "\n",
    "\n",
    "# return fft results and peaks indexes\n",
    "def get_data(signal):\n",
    "    xf, yf = get_fxy(signal)\n",
    "    indexes = get_indexes(xf, yf, len(signal))\n",
    "    return xf, yf, indexes\n",
    "\n",
    "#compare frequencies as L1\n",
    "def get_distance(a, b, peaks1, peaks2):\n",
    "    #TODO add peaks magnitude comparison\n",
    "    return np.sum(np.abs(a-b))\n",
    "\n",
    "\n",
    "def do_task1():\n",
    "    with open('signals.npy', 'rb') as f:\n",
    "        signals = np.load(f)\n",
    "        \n",
    "    with open('mixtures.npy', 'rb') as f:\n",
    "        y = np.load(f)\n",
    "      \n",
    "    signal_data = []\n",
    "    \n",
    "    #gather info about signals ahead (results of fft and peak indexes)\n",
    "    for signal in signals:\n",
    "        signal_data.append(get_data(signal))\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(y)):\n",
    "        yi = y[i]\n",
    "        #fft results and peak indexes\n",
    "        xf_y, yf_y, ind_y = get_data(yi)\n",
    "        #peak frequencies\n",
    "        f_y = xf_y[ind_y]\n",
    "\n",
    "        diff = 30000\n",
    "        #this index will contain index of most appropriate pure signal\n",
    "        index = 0\n",
    "        for j in range(len(signals)):\n",
    "            #get pure signal data (gathered before)\n",
    "            xf_s, yf_s, ind_s = signal_data[j]        \n",
    "            #top frequencies for pure signal\n",
    "            f_s = xf_s[ind_s]        \n",
    "            # if mixed signal contains less top freq than pure signal, give up with this \n",
    "            if (len(ind_y) <= len(ind_s)):\n",
    "                break\n",
    "            #compare top frequencies of mixture and pure signal\n",
    "            new_diff = get_distance(f_y[0:len(ind_s)],f_s, None, None)  \n",
    "            #select the most appropriate\n",
    "            if (new_diff < diff):\n",
    "                index = j\n",
    "                diff=new_diff\n",
    "                \n",
    "        #this assert for my test data\n",
    "        #assert  i%100 == index, f\"Not correct for {i} and {index}\"\n",
    "         \n",
    "        # So we know pure signal now, let's do rest of computations    \n",
    "        xf_s, yf_s, ind_s = signal_data[index]\n",
    "        freq_num = len(ind_s)    \n",
    "        phase_shift = np.abs(np.angle(yf_s[ind_s[0]]) - np.angle(yf_y[ind_y][0]))\n",
    "        signal_max_power = np.max(np.abs(yf_y[ind_y[0:freq_num]]))\n",
    "        noise_max_power = np.max(np.abs(yf_y[ind_y[freq_num:]]))\n",
    "        snr_in_dB = 10 * np.log10(signal_max_power/noise_max_power)                        \n",
    "\n",
    "        result.append((index, phase_shift, snr_in_dB))\n",
    "        \n",
    "        \n",
    "do_task1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task2.\n",
    "\n",
    "We will use chi2 criterion.\n",
    "<ol>\n",
    "<li>For each distribution: we split axis into k intervals, in each interval area under density curve will be the same (point corresponds to percentiles); </li>\n",
    "<li>For each distribution: we count how many values would contain each interval for given n (size of the array), so these are our expected values;</li> \n",
    "<li>In practice: results of the (1) - computed intervals I would add in dictinary to don't compute for each k; </li>\n",
    "<li>Then we need compute how many values from our array (each of 1024 arrays) will be in each interval (k intervals for each distribution);</li> \n",
    "<li>we make chi2 for observed values (step4) and expected values(step2) and make argmax to find the best answer. As an improvement we could use some threshold how much we sure about answer (say pvalue > 0.8)...</li>\n",
    "</ol>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our test array\n",
    "dim0 = 1024\n",
    "#how many distributions we have\n",
    "distr_num = 4\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "norm_distr = sps.norm()\n",
    "expon_distr = sps.expon()              \n",
    "chi_distr = sps.chi2(df=1) \n",
    "gamma_distr = sps.gamma(scale=2,a=3) #scale==teta, k=?a (not sure)\n",
    "distrs = [norm_distr, expon_distr, chi_distr, gamma_distr]\n",
    "distr_num = len(distrs)\n",
    "\n",
    "def get_test_data():    \n",
    "    # min value for n is 10 as we need at least 2 interval with 5 elements\n",
    "    n=1000 #length of one array\n",
    "    #randomly choose distribution for test data\n",
    "    distr_nums = np.random.randint(0, high = distr_num, size=dim0)\n",
    "    test_data = []\n",
    "    for i in range(dim0):\n",
    "        test_data.append(distrs[distr_nums[i]].rvs(n))\n",
    "    test_data = np.array(test_data)\n",
    "    return distr_nums, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "distr_nums, test_data = get_test_data()\n",
    "\n",
    "# values for k(number of intervals) were taken from there: https://ami.nstu.ru/~headrd/seminar/publik_html/Z_lab_8.htm\n",
    "def get_k(n):\n",
    "    assert n>=10\n",
    "    if n >= 10 and n < 40:\n",
    "        return n/5\n",
    "    elif n >= 40 and n <100:\n",
    "        return 8\n",
    "    elif n >= 100 and n < 500:\n",
    "        return 10\n",
    "    elif n >= 500 and n <1000:\n",
    "        return 12\n",
    "    else:\n",
    "        return 20\n",
    "\n",
    "\n",
    "def define_distr(test_data):\n",
    "    n = test_data.shape[1]\n",
    "    k=get_k(n)   \n",
    "    points = []\n",
    "    step = 1/k # area under density curve in each interval\n",
    "    \n",
    "    #compute interval boundaries, as we don't know n ahead and consequenly k, we compute it each time \n",
    "    #(but could use hash for more frequently used )\n",
    "    for i in range(1, k):\n",
    "        points.append([distr.ppf(i * step) for distr in distrs])\n",
    "    points = np.array(points).T\n",
    "    #number of expected values\n",
    "    expected = [n/k]*k\n",
    "\n",
    "    sums =[]\n",
    "    test_data = np.sort(test_data, axis=1) \n",
    "    #for each arry\n",
    "    for i in range(dim0):    \n",
    "        #find indexes corresponding to interval boundaries\n",
    "        indexes = np.searchsorted(test_data[i],points.reshape(-1),side='right')\n",
    "        indexes = indexes.reshape(-1, k-1) \n",
    "        #count how many values in each interval\n",
    "        sums.append([[len(x) for x in np.split(test_data[i], ind)] for ind in indexes])\n",
    "    sums = np.array(sums)      \n",
    "    return sps.chisquare(sums, expected, axis=2).pvalue.argmax(axis=1)\n",
    "\n",
    "\n",
    "#assert np.array_equal(define_distr(test_data),np.array(distr_nums)), \"May be something with test data? :)\"\n",
    "result = define_distr(test_data)\n",
    "num_of_correct_answers = np.where(define_distr(test_data)== np.array(distr_nums), 1, 0).sum()\n",
    "print(\"Accuracy: \", num_of_correct_answers/dim0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
